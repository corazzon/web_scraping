{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d5c44d-f3c6-4301-a467-de03634f3572",
   "metadata": {},
   "source": [
    "https://china.huanqiu.com/\n",
    "\n",
    "\n",
    " \n",
    "### Request URL:\n",
    "https://china.huanqiu.com/api/list?node=%22/e3pmh1nnq/e3pmh1obd%22,%22/e3pmh1nnq/e3pn61c2g%22,%22/e3pmh1nnq/e3pn6eiep%22,%22/e3pmh1nnq/e3pra70uk%22,%22/e3pmh1nnq/e5anm31jb%22,%22/e3pmh1nnq/e7tl4e309%22&offset=24&limit=24\n",
    "Request Method:\n",
    "GET\n",
    "\n",
    "\n",
    "### Payload\n",
    "node=%22/e3pmh1nnq/e3pmh1obd%22,%22/e3pmh1nnq/e3pn61c2g%22,%22/e3pmh1nnq/e3pn6eiep%22,%22/e3pmh1nnq/e3pra70uk%22,%22/e3pmh1nnq/e5anm31jb%22,%22/e3pmh1nnq/e7tl4e309%22&offset=24&limit=24\n",
    "\n",
    "\n",
    "### 응답 데이터 일부 예시 입니다. \n",
    "\n",
    "{\n",
    "    \"list\": [{\n",
    "    \t\"aid\": \"4IiO9qGa9i5\",\n",
    "    \t\"title\": \"最新版肉制品生产监督检查操作指南来了\",\n",
    "    \t\"summary\": \"指导基层市场监管人员熟练掌握肉制品生产企业检查要点和检查方法，切实提升监督检查水平，守稳筑牢食品安全底线。\",\n",
    "    \t\"addltype\": \"normal\",\n",
    "    \t\"typedata\":{\"audio\":{\"members\":[]},\"gallery\":{\"members\":[]},\"video\":{\"members\":[]}},\n",
    "    \t\"source\" :{\"name\":\"央视新闻客户端\",\"url\":\"https:\\/\\/content-static.cctvnews.cctv.com\\/snow-book\\/index.html?item_id=12991606494210524639&toc_style_id=feeds_default&track_id=026217CF-94F2-433D-8F1C-91F327CDA784_743338808688&share_to=wechat\"},\n",
    "    \t\"ext_displaytime\": \"\",\n",
    "    \t\"ext_defertime\":\"\",\n",
    "    \t\"ctime\": \"1721647769902\",\n",
    "    \t\"xtime\": \"1721647769902\",\n",
    "    \t\"cover\" : \"\",\n",
    "    \t\"host\" : \"china.huanqiu.com\",\n",
    "\t\t\"ext-serious\" : \"1\",\n",
    "\t\t\"ext-weight\" : \"50\"\n",
    "    },{\n",
    "    \t\"aid\": \"4IiNXLfI4MG\",\n",
    "    \t\"title\": \"人民观察｜经典与创新碰撞出吉林文旅融合“新火花”\",\n",
    "    \t\"summary\": \"盛夏傍晚时分，还未走到长影世纪城“山海奇妙夜”的大门，记者就被景区门前的一排花灯所吸引。\",\n",
    "    \t\"addltype\": \"normal\",\n",
    "    \t\"typedata\":{\"audio\":{\"members\":[]},\"gallery\":{\"members\":[{\"desc\":null,\"height\":566,\"id\":\"a1i9vr_759728\",\"mime\":\"image\\/jpg\",\"size\":159.71,\"\n",
    "\n",
    "\n",
    "\n",
    "* 판다스 데이터프레임으로 여러 페이지의 뉴스 기사를 수집할 수 있도록 작성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e2e9be-431c-448f-98a1-937ecfd681db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "from requests.exceptions import RequestException\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://china.huanqiu.com/api/list\"\n",
    "NODES = \"\\\"/e3pmh1nnq/e3pmh1obd\\\",\\\"/e3pmh1nnq/e3pn61c2g\\\",\\\"/e3pmh1nnq/e3pn6eiep\\\",\\\"/e3pmh1nnq/e3pra70uk\\\",\\\"/e3pmh1nnq/e5anm31jb\\\",\\\"/e3pmh1nnq/e7tl4e309\\\"\"\n",
    "LIMIT = 24\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 5\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class HuanqiuScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "\n",
    "    def fetch_data(self, offset: int) -> Optional[Dict]:\n",
    "        params = {\n",
    "            'node': NODES,\n",
    "            'offset': offset,\n",
    "            'limit': LIMIT\n",
    "        }\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                response = self.session.get(BASE_URL, params=params, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                return response.json()\n",
    "            except RequestException as e:\n",
    "                logger.warning(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt < MAX_RETRIES - 1:\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                else:\n",
    "                    logger.error(f\"Max retries reached. Skipping offset {offset}\")\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_articles(data: Dict) -> List[Dict]:\n",
    "        if not data or 'list' not in data:\n",
    "            return []\n",
    "        \n",
    "        return [{\n",
    "            'aid': article.get('aid', ''),\n",
    "            'title': article.get('title', ''),\n",
    "            'summary': article.get('summary', ''),\n",
    "            'source_name': article.get('source', {}).get('name', ''),\n",
    "            'source_url': article.get('source', {}).get('url', ''),\n",
    "            'ctime': article.get('ctime', ''),\n",
    "            'xtime': article.get('xtime', ''),\n",
    "            'host': article.get('host', ''),\n",
    "            'ext_serious': article.get('ext-serious', ''),\n",
    "            'ext_weight': article.get('ext-weight', '')\n",
    "        } for article in data['list']]\n",
    "\n",
    "    def collect_articles(self, pages: int) -> List[Dict]:\n",
    "        all_articles = []\n",
    "        for i in range(pages):\n",
    "            offset = i * LIMIT\n",
    "            data = self.fetch_data(offset)\n",
    "            if data:\n",
    "                articles = self.parse_articles(data)\n",
    "                all_articles.extend(articles)\n",
    "            else:\n",
    "                logger.warning(f\"No data retrieved for page {i+1}\")\n",
    "        return all_articles\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame) -> str:\n",
    "    filename = f\"huanqiu_articles_{time.strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    return filename\n",
    "\n",
    "def main():\n",
    "    scraper = HuanqiuScraper()\n",
    "    try:\n",
    "        pages_to_collect = int(input(\"Enter the number of pages to collect: \"))\n",
    "        articles_data = scraper.collect_articles(pages_to_collect)\n",
    "\n",
    "        if not articles_data:\n",
    "            logger.error(\"No articles collected. Exiting.\")\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(articles_data)\n",
    "        logger.info(f\"Collected {len(df)} articles.\")\n",
    "        \n",
    "        filename = save_to_csv(df)\n",
    "        logger.info(f\"Data saved to {filename}\")\n",
    "\n",
    "        logger.info(\"\\nFirst few rows of the collected data:\")\n",
    "        df.head()\n",
    "\n",
    "    except ValueError:\n",
    "        logger.error(\"Invalid input. Please enter a valid number of pages.\")\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddbfcde-81c6-40bc-b768-2834511f476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def load_csv_files(directory='.'):\n",
    "    # 지정된 디렉토리에서 \"huanqiu_articles_\"로 시작하는 모든 CSV 파일 찾기\n",
    "    csv_files = glob.glob(os.path.join(directory, 'huanqiu_articles_*.csv'))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found.\")\n",
    "        return None\n",
    "\n",
    "    # 모든 CSV 파일을 하나의 데이터프레임으로 로드\n",
    "    df_list = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df['file_name'] = os.path.basename(file)  # 파일 이름 추가\n",
    "        df_list.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a66a28e-79dd-45ab-9f32-98deacdf72e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_csv_files()\n",
    "if df is not None:\n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(f\"Shape of the combined dataframe: {df.shape}\")\n",
    "    \n",
    "    print(\"\\nAnalysis Results:\")\n",
    "    analyze_data(df)\n",
    "    \n",
    "    # 결과 저장\n",
    "    output_file = f\"huanqiu_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nCombined data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d746ce52-156a-485c-8b0f-5440b2a4c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of articles: {len(df)}\")\n",
    "\n",
    "# 중복 제거 후 고유한 기사 수\n",
    "unique_articles = df.drop_duplicates(subset=['aid'])\n",
    "print(f\"Number of unique articles: {len(unique_articles)}\")\n",
    "\n",
    "# 가장 많은 기사를 가진 상위 5개 출처\n",
    "top_sources = df['source_name'].value_counts().head()\n",
    "print(\"\\nTop 5 sources:\")\n",
    "print(top_sources)\n",
    "\n",
    "# 시간대별 기사 수\n",
    "df['datetime'] = pd.to_datetime(df['ctime'].astype(float), unit='ms')\n",
    "df['date'] = df['datetime'].dt.date\n",
    "articles_by_date = df['date'].value_counts().sort_index()\n",
    "print(\"\\nArticles by date:\")\n",
    "print(articles_by_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6e2c2-f2f2-44e2-8dbe-18078e4f8943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 많이 등장하는 키워드 (제목 기준)\n",
    "df['title'] = df['title'].fillna('')  # NaN 값을 빈 문자열로 대체\n",
    "df['title_words'] = df['title'].apply(lambda x: x.split() if isinstance(x, str) else [])\n",
    "all_words = [word for words in df['title_words'] for word in words]\n",
    "word_counts = pd.Series(all_words).value_counts()\n",
    "print(\"\\nTop 10 keywords in titles:\")\n",
    "print(word_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ead92-5026-4238-be0c-ad9a75485a2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d30ea56-214d-4158-981d-45209725b514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
