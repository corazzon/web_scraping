{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d5c44d-f3c6-4301-a467-de03634f3572",
   "metadata": {},
   "source": [
    "https://china.huanqiu.com/\n",
    "\n",
    "\n",
    " \n",
    "### Request URL:\n",
    "https://china.huanqiu.com/api/list?node=%22/e3pmh1nnq/e3pmh1obd%22,%22/e3pmh1nnq/e3pn61c2g%22,%22/e3pmh1nnq/e3pn6eiep%22,%22/e3pmh1nnq/e3pra70uk%22,%22/e3pmh1nnq/e5anm31jb%22,%22/e3pmh1nnq/e7tl4e309%22&offset=24&limit=24\n",
    "Request Method:\n",
    "GET\n",
    "\n",
    "\n",
    "### Payload\n",
    "node=%22/e3pmh1nnq/e3pmh1obd%22,%22/e3pmh1nnq/e3pn61c2g%22,%22/e3pmh1nnq/e3pn6eiep%22,%22/e3pmh1nnq/e3pra70uk%22,%22/e3pmh1nnq/e5anm31jb%22,%22/e3pmh1nnq/e7tl4e309%22&offset=24&limit=24\n",
    "\n",
    "\n",
    "### 응답 데이터 일부 예시 입니다. \n",
    "\n",
    "{\n",
    "    \"list\": [{\n",
    "    \t\"aid\": \"4IiO9qGa9i5\",\n",
    "    \t\"title\": \"最新版肉制品生产监督检查操作指南来了\",\n",
    "    \t\"summary\": \"指导基层市场监管人员熟练掌握肉制品生产企业检查要点和检查方法，切实提升监督检查水平，守稳筑牢食品安全底线。\",\n",
    "    \t\"addltype\": \"normal\",\n",
    "    \t\"typedata\":{\"audio\":{\"members\":[]},\"gallery\":{\"members\":[]},\"video\":{\"members\":[]}},\n",
    "    \t\"source\" :{\"name\":\"央视新闻客户端\",\"url\":\"https:\\/\\/content-static.cctvnews.cctv.com\\/snow-book\\/index.html?item_id=12991606494210524639&toc_style_id=feeds_default&track_id=026217CF-94F2-433D-8F1C-91F327CDA784_743338808688&share_to=wechat\"},\n",
    "    \t\"ext_displaytime\": \"\",\n",
    "    \t\"ext_defertime\":\"\",\n",
    "    \t\"ctime\": \"1721647769902\",\n",
    "    \t\"xtime\": \"1721647769902\",\n",
    "    \t\"cover\" : \"\",\n",
    "    \t\"host\" : \"china.huanqiu.com\",\n",
    "\t\t\"ext-serious\" : \"1\",\n",
    "\t\t\"ext-weight\" : \"50\"\n",
    "    },{\n",
    "    \t\"aid\": \"4IiNXLfI4MG\",\n",
    "    \t\"title\": \"人民观察｜经典与创新碰撞出吉林文旅融合“新火花”\",\n",
    "    \t\"summary\": \"盛夏傍晚时分，还未走到长影世纪城“山海奇妙夜”的大门，记者就被景区门前的一排花灯所吸引。\",\n",
    "    \t\"addltype\": \"normal\",\n",
    "    \t\"typedata\":{\"audio\":{\"members\":[]},\"gallery\":{\"members\":[{\"desc\":null,\"height\":566,\"id\":\"a1i9vr_759728\",\"mime\":\"image\\/jpg\",\"size\":159.71,\"\n",
    "\n",
    "\n",
    "\n",
    "* 판다스 데이터프레임으로 여러 페이지의 뉴스 기사를 수집할 수 있도록 작성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f07f7da-6970-4898-9949-a06d31271d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "from requests.exceptions import RequestException\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e2e9be-431c-448f-98a1-937ecfd681db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 상수 설정\n",
    "BASE_URL = \"https://china.huanqiu.com/api/list\"\n",
    "NODES = \"\\\"/e3pmh1nnq/e3pmh1obd\\\",\\\"/e3pmh1nnq/e3pn61c2g\\\",\\\"/e3pmh1nnq/e3pn6eiep\\\",\\\"/e3pmh1nnq/e3pra70uk\\\",\\\"/e3pmh1nnq/e5anm31jb\\\",\\\"/e3pmh1nnq/e7tl4e309\\\"\"\n",
    "LIMIT = 24\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 5\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class HuanqiuScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "\n",
    "    def fetch_data(self, offset: int) -> Optional[Dict]:\n",
    "        params = {\n",
    "            'node': NODES,\n",
    "            'offset': offset,\n",
    "            'limit': LIMIT\n",
    "        }\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                response = self.session.get(BASE_URL, params=params, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                return response.json()\n",
    "            except RequestException as e:\n",
    "                logger.warning(f\"시도 {attempt + 1} 실패: {e}\")\n",
    "                if attempt < MAX_RETRIES - 1:\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                else:\n",
    "                    logger.error(f\"최대 재시도 횟수 도달. 오프셋 {offset} 건너뜀\")\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_articles(data: Dict) -> List[Dict]:\n",
    "        if not data or 'list' not in data:\n",
    "            return []\n",
    "        \n",
    "        return [{\n",
    "            'aid': article.get('aid', ''),\n",
    "            'title': article.get('title', ''),\n",
    "            'summary': article.get('summary', ''),\n",
    "            'source_name': article.get('source', {}).get('name', ''),\n",
    "            'source_url': article.get('source', {}).get('url', ''),\n",
    "            'ctime': article.get('ctime', ''),\n",
    "            'xtime': article.get('xtime', ''),\n",
    "            'host': article.get('host', ''),\n",
    "            'ext_serious': article.get('ext-serious', ''),\n",
    "            'ext_weight': article.get('ext-weight', '')\n",
    "        } for article in data['list']]\n",
    "\n",
    "    def collect_articles(self, pages: int) -> List[Dict]:\n",
    "        all_articles = []\n",
    "        for i in range(pages):\n",
    "            offset = i * LIMIT\n",
    "            data = self.fetch_data(offset)\n",
    "            if data:\n",
    "                articles = self.parse_articles(data)\n",
    "                all_articles.extend(articles)\n",
    "            else:\n",
    "                logger.warning(f\"페이지 {i+1}에서 데이터 수집 실패\")\n",
    "        return all_articles\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame) -> str:\n",
    "    filename = f\"huanqiu_articles_{time.strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    return filename\n",
    "\n",
    "def load_csv_files() -> Optional[pd.DataFrame]:\n",
    "    csv_files = [file for file in os.listdir() if file.startswith('huanqiu_articles_') and file.endswith('.csv')]\n",
    "    if not csv_files:\n",
    "        logger.error(\"CSV 파일을 찾을 수 없습니다.\")\n",
    "        return None\n",
    "    \n",
    "    df_list = []\n",
    "    for file in csv_files:\n",
    "        df_list.append(pd.read_csv(file))\n",
    "    \n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "def analyze_data(df: pd.DataFrame):\n",
    "    # 여기서 데이터 분석 로직을 추가할 수 있습니다.\n",
    "    print(f\"총 기사 수: {len(df)}\")\n",
    "    print(f\"첫 기사 제목: {df.iloc[0]['title'] if not df.empty else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d63b54-7fa2-4ec8-b51c-bc8852be3dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    scraper = HuanqiuScraper()\n",
    "    try:\n",
    "        pages_to_collect = int(input(\"수집할 페이지 수를 입력하세요: \"))\n",
    "        articles_data = scraper.collect_articles(pages_to_collect)\n",
    "\n",
    "        if not articles_data:\n",
    "            logger.error(\"수집된 기사가 없습니다. 종료합니다.\")\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(articles_data)\n",
    "        logger.info(f\"{len(df)}개의 기사를 수집했습니다.\")\n",
    "        \n",
    "        filename = save_to_csv(df)\n",
    "        logger.info(f\"데이터가 {filename} 파일에 저장되었습니다.\")\n",
    "\n",
    "        # CSV 파일 로드 및 분석\n",
    "        df = load_csv_files()\n",
    "        if df is not None:\n",
    "            print(\"데이터 로드 성공.\")\n",
    "            print(f\"결합된 데이터프레임의 크기: {df.shape}\")\n",
    "            \n",
    "            print(\"\\n분석 결과:\")\n",
    "            analyze_data(df)\n",
    "            \n",
    "            # 결과 저장\n",
    "            output_file = f\"huanqiu_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "            df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "            print(f\"\\n결합된 데이터가 {output_file} 파일에 저장되었습니다.\")\n",
    "\n",
    "    except ValueError:\n",
    "        logger.error(\"잘못된 입력입니다. 유효한 페이지 수를 입력하세요.\")\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"예상치 못한 오류 발생: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddbfcde-81c6-40bc-b768-2834511f476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def load_csv_files(directory='.'):\n",
    "    # 지정된 디렉토리에서 \"huanqiu_articles_\"로 시작하는 모든 CSV 파일 찾기\n",
    "    csv_files = glob.glob(os.path.join(directory, 'huanqiu_articles_*.csv'))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found.\")\n",
    "        return None\n",
    "\n",
    "    # 모든 CSV 파일을 하나의 데이터프레임으로 로드\n",
    "    df_list = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df['file_name'] = os.path.basename(file)  # 파일 이름 추가\n",
    "        df_list.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a66a28e-79dd-45ab-9f32-98deacdf72e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_csv_files()\n",
    "if df is not None:\n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(f\"Shape of the combined dataframe: {df.shape}\")\n",
    "    \n",
    "    print(\"\\nAnalysis Results:\")\n",
    "    analyze_data(df)\n",
    "    \n",
    "    # 결과 저장\n",
    "    output_file = f\"huanqiu_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nCombined data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d746ce52-156a-485c-8b0f-5440b2a4c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of articles: {len(df)}\")\n",
    "\n",
    "# 중복 제거 후 고유한 기사 수\n",
    "unique_articles = df.drop_duplicates(subset=['aid'])\n",
    "print(f\"Number of unique articles: {len(unique_articles)}\")\n",
    "\n",
    "# 가장 많은 기사를 가진 상위 5개 출처\n",
    "top_sources = df['source_name'].value_counts().head()\n",
    "print(\"\\nTop 5 sources:\")\n",
    "print(top_sources)\n",
    "\n",
    "# 시간대별 기사 수\n",
    "df['datetime'] = pd.to_datetime(df['ctime'].astype(float), unit='ms')\n",
    "df['date'] = df['datetime'].dt.date\n",
    "articles_by_date = df['date'].value_counts().sort_index()\n",
    "print(\"\\nArticles by date:\")\n",
    "print(articles_by_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6e2c2-f2f2-44e2-8dbe-18078e4f8943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 많이 등장하는 키워드 (제목 기준)\n",
    "df['title'] = df['title'].fillna('')  # NaN 값을 빈 문자열로 대체\n",
    "df['title_words'] = df['title'].apply(lambda x: x.split() if isinstance(x, str) else [])\n",
    "all_words = [word for words in df['title_words'] for word in words]\n",
    "word_counts = pd.Series(all_words).value_counts()\n",
    "print(\"\\nTop 10 keywords in titles:\")\n",
    "print(word_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ead92-5026-4238-be0c-ad9a75485a2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d30ea56-214d-4158-981d-45209725b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d677ec2f-72c9-4f49-a7ed-ffe276de816b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_text = \"\\n\\n\".join(df['title'].fillna(\"\\n\") + \"\\n\" + df['summary'].fillna(\"\\n\"))\n",
    "combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9990dd73-e10a-4e42-91f0-618268d8e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title and summary with newline characters\n",
    "# combined_text = \"\\n\".join(df['title'] + \"\\n\" + df['summary'])\n",
    "\n",
    "# Save to a text file\n",
    "file_path = \"china_huanqiu_com_combined_text.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(combined_text)\n",
    "\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55f5545-67e5-4c5f-b710-a212132b16f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
