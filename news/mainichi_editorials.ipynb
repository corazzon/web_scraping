{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a76f1d19-0b86-445d-839b-953c6b218685",
   "metadata": {},
   "source": [
    "https://mainichi.jp/editorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f521bff-97e3-4e5e-9a4f-ab51f5a9b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "def create_db_and_schema(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS editorials (\n",
    "            title TEXT,\n",
    "            link TEXT,\n",
    "            text_ellipsis TEXT,\n",
    "            date TEXT,\n",
    "            content TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def fetch_mainichi_editorials(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        articles = soup.find_all('li')\n",
    "        editorial_list = []\n",
    "        for article in articles:\n",
    "            title_tag = article.select_one('.articlelist-title')\n",
    "            if not title_tag:\n",
    "                continue  # title이 없는 경우 해당 기사를 건너뜀\n",
    "            title = title_tag.text.strip()\n",
    "            link = article.find('a')['href']\n",
    "            full_link = f\"https://mainichi.jp{link}\"\n",
    "            date_tag = article.select_one('.articletag-date')\n",
    "            text_ellipsis = article.select_one('.text-ellipsis-2')\n",
    "            date = date_tag.text.strip() if date_tag else \"N/A\"\n",
    "            editorial_list.append({\n",
    "                'title': title,\n",
    "                'link': full_link,\n",
    "                'text_ellipsis': text_ellipsis.text.strip() if text_ellipsis else \"N/A\",\n",
    "                'date': date\n",
    "            })\n",
    "        return editorial_list\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_article_content(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        article_body = soup.select(\"#main > div > article\")\n",
    "        if article_body:\n",
    "            paragraphs = article_body[0].find_all('p')\n",
    "            content = '\\n'.join([p.get_text(strip=True) for p in paragraphs])\n",
    "        else:\n",
    "            content = \"본문을 찾을 수 없습니다.\"\n",
    "        return content\n",
    "    except requests.RequestException as e:\n",
    "        return f\"요청 중 오류 발생: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"예상치 못한 오류 발생: {e}\"\n",
    "\n",
    "def save_to_db(editorials, db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    editorials_df = pd.DataFrame(editorials)\n",
    "    editorials_df.to_sql('editorials', conn, if_exists='append', index=False)\n",
    "    conn.close()\n",
    "\n",
    "def collect_all_editorials(base_url, db_path):\n",
    "    page = 1  # 시작 페이지 설정\n",
    "    while True:\n",
    "        print(f\"Collecting page {page}...\")  # 현재 페이지 수집 중임을 출력\n",
    "        url = f\"{base_url}{page}?&_=1722162257351\"\n",
    "        editorials = fetch_mainichi_editorials(url)\n",
    "        if not editorials:\n",
    "            break\n",
    "        for editorial in editorials:\n",
    "            content = fetch_article_content(editorial['link'])\n",
    "            editorial['content'] = content\n",
    "            save_to_db([editorial], db_path)\n",
    "        page += 1\n",
    "\n",
    "# URL 설정\n",
    "base_url = \"https://mainichi.jp/editorial/\"\n",
    "# DB 경로 설정\n",
    "db_path = 'mainichi_editorials.db'\n",
    "\n",
    "# DB와 스키마 생성\n",
    "create_db_and_schema(db_path)\n",
    "\n",
    "# 사설 목록 수집 및 DB 저장\n",
    "collect_all_editorials(base_url, db_path)\n",
    "\n",
    "# SQLite DB 확인\n",
    "def verify_db(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df = pd.read_sql('SELECT * FROM editorials', conn)\n",
    "    display(df)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "# DB 검증\n",
    "verify_db(db_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c1947-d39e-47bd-a8ec-27a71bd668e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLite 데이터베이스 파일명\n",
    "DB_NAME = 'mainichi_editorials.db'\n",
    "\n",
    "# 데이터베이스 연결 및 커서 생성\n",
    "conn = sqlite3.connect(DB_NAME)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "# 데이터프레임으로 테이블 데이터 불러오기\n",
    "df = pd.read_sql('SELECT * FROM editorials', conn)\n",
    "conn.close()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e739fb6d-de90-4bfd-9770-d531cf93be6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
