{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d005de3d-e0d4-48f3-89f6-dd537354387c",
   "metadata": {},
   "source": [
    "## prompt\n",
    "\n",
    "https://www.samsung.com/fr/ 와 https://www.samsung.com/ca_fr/ 사이트의 링크를 순회하면서 html 문서를 읽어와서 텍스트 파일만 수집하는 코드를 작성하고 해당 사이트에 있는 url , text,  항목으로  sqlite 데이터 베이스를 만들어서 저장하고 수집하는 과정을 확인하는 코드도 작성할것 테이블에 ca, ca_fr을 구분하는 컬럼도 추가할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2eb7a3-5c8f-454c-9893-18ec16dc904f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def create_database():\n",
    "    conn = sqlite3.connect('samsung_data.db')\n",
    "    c = conn.cursor()\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS samsung_pages\n",
    "                 (url TEXT PRIMARY KEY, text TEXT, country TEXT)''')\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "def insert_data(conn, url, text, country):\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"INSERT OR REPLACE INTO samsung_pages (url, text, country) VALUES (?, ?, ?)\",\n",
    "              (url, text, country))\n",
    "    conn.commit()\n",
    "\n",
    "def crawl_website(base_url, country):\n",
    "    visited = set()\n",
    "    to_visit = [base_url]\n",
    "    \n",
    "    conn = create_database()\n",
    "\n",
    "    while to_visit:\n",
    "        current_url = to_visit.pop(0)\n",
    "        if current_url in visited:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # Extract text\n",
    "                text = ' '.join([p.get_text(strip=True) for p in soup.find_all('p')])\n",
    "                \n",
    "                # Insert data into database\n",
    "                insert_data(conn, current_url, text, country)\n",
    "                \n",
    "                print(f\"Crawled: {current_url}\")\n",
    "\n",
    "                # Find new links\n",
    "                for link in soup.find_all('a', href=True):\n",
    "                    new_url = urljoin(base_url, link['href'])\n",
    "                    if new_url.startswith(base_url) and new_url not in visited:\n",
    "                        to_visit.append(new_url)\n",
    "\n",
    "                visited.add(current_url)\n",
    "            \n",
    "            time.sleep(1)  # Be respectful to the server\n",
    "        except Exception as e:\n",
    "            print(f\"Error crawling {current_url}: {e}\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "def verify_data():\n",
    "    conn = sqlite3.connect('samsung_data.db')\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # Count total entries\n",
    "    c.execute(\"SELECT COUNT(*) FROM samsung_pages\")\n",
    "    total_entries = c.fetchone()[0]\n",
    "    print(f\"Total entries in database: {total_entries}\")\n",
    "\n",
    "    # Count entries for each country\n",
    "    c.execute(\"SELECT country, COUNT(*) FROM samsung_pages GROUP BY country\")\n",
    "    country_counts = c.fetchall()\n",
    "    for country, count in country_counts:\n",
    "        print(f\"Entries for {country}: {count}\")\n",
    "\n",
    "    # Display a few sample entries\n",
    "    c.execute(\"SELECT * FROM samsung_pages LIMIT 5\")\n",
    "    sample_entries = c.fetchall()\n",
    "    print(\"\\nSample entries:\")\n",
    "    for entry in sample_entries:\n",
    "        print(f\"URL: {entry[0]}\")\n",
    "        print(f\"Text preview: {entry[1][:100]}...\")\n",
    "        print(f\"Country: {entry[2]}\")\n",
    "        print(\"---\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    france_url = \"https://www.samsung.com/fr/\"\n",
    "    canada_fr_url = \"https://www.samsung.com/ca_fr/\"\n",
    "\n",
    "    print(\"Crawling Samsung France website...\")\n",
    "    crawl_website(france_url, \"fr\")\n",
    "\n",
    "    print(\"\\nCrawling Samsung Canada (French) website...\")\n",
    "    crawl_website(canada_fr_url, \"ca_fr\")\n",
    "\n",
    "    print(\"\\nVerifying collected data...\")\n",
    "    verify_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54233de3-a1c1-4d00-adda-2109d7cc3bf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nCrawling Samsung Canada (French) website...\")\n",
    "crawl_website(canada_fr_url, \"ca_fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c715ba-15a7-40d3-96d4-f255cc74a9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
