{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd419725-3d86-4691-a614-c1f3eca1ca27",
   "metadata": {},
   "source": [
    "[컴퓨터 공학 - 예스24](https://www.yes24.com/24/Category/Display/001001003031?FetchSize=100&PageNumber=71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fb64c1-e51c-41a1-9fb7-5343e930d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from requests.exceptions import RequestException\n",
    "import re\n",
    "\n",
    "def retry_request(url, max_retries=5, base_delay=1):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except RequestException as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise e\n",
    "            delay = base_delay * (2 ** attempt) + random.uniform(0, 1)\n",
    "            print(f\"Request failed. Retrying in {delay:.2f} seconds...\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "def get_last_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # \"끝\" 버튼을 찾습니다.\n",
    "    end_button = soup.select_one('.yesUI_pagenS a.bgYUI.end')\n",
    "    \n",
    "    if end_button:\n",
    "        # \"끝\" 버튼의 href 속성에서 PageNumber 값을 추출합니다.\n",
    "        href = end_button.get('href', '')\n",
    "        match = re.search(r'PageNumber=(\\d+)', href)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    \n",
    "    # \"끝\" 버튼을 찾지 못하거나 PageNumber를 추출하지 못한 경우\n",
    "    # 페이지 번호들을 찾아 그 중 가장 큰 값을 반환합니다.\n",
    "    page_numbers = [int(a.text) for a in soup.select('.yesUI_pagenS a.num') if a.text.isdigit()]\n",
    "    return max(page_numbers) if page_numbers else 1\n",
    "\n",
    "\n",
    "def scrape_page(url):\n",
    "    response = retry_request(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    books = soup.select('.goods_info')\n",
    "    \n",
    "    data = []\n",
    "    for book in books:\n",
    "        title = book.select_one('.goods_name a').text.strip() if book.select_one('.goods_name a') else 'N/A'\n",
    "        author = book.select_one('.goods_auth a').text.strip() if book.select_one('.goods_auth a') else 'N/A'\n",
    "        publisher = book.select_one('.goods_pub').text.strip() if book.select_one('.goods_pub') else 'N/A'\n",
    "        pub_date = book.select_one('.goods_date').text.strip() if book.select_one('.goods_date') else 'N/A'\n",
    "        price = book.select_one('.goods_price .yes_b').text.strip() if book.select_one('.goods_price .yes_b') else 'N/A'\n",
    "        discount = book.select_one('.goods_benefit').text.strip() if book.select_one('.goods_benefit') else 'N/A'\n",
    "        rating = book.select_one('.gd_rating .yes_b').text.strip() if book.select_one('.gd_rating .yes_b') else 'N/A'\n",
    "        review_count = book.select_one('.gd_reviewCount .txC_blue').text.strip() if book.select_one('.gd_reviewCount .txC_blue') else 'N/A'\n",
    "        detail_url = 'https://www.yes24.com' + book.select_one('.goods_name a')['href'] if book.select_one('.goods_name a') else 'N/A'\n",
    "        description = book.select_one('.goods_read').text.strip() if book.select_one('.goods_read') else 'N/A'\n",
    "        \n",
    "        data.append((title, author, publisher, pub_date, price, discount, rating, review_count, detail_url, description))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1079140e-0f23-4bbf-85c5-9a2d5ede70e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(cat_id):\n",
    "    base_url = f\"https://www.yes24.com/24/Category/Display/{cat_id}?FetchSize=300&PageNumber={}\"\n",
    "    \n",
    "    # Get total number of pages\n",
    "    total_pages = get_last_page(base_url.format(1))\n",
    "    print(f\"Total pages: {total_pages}\")\n",
    "    \n",
    "    # Create SQLite database and table\n",
    "    conn = sqlite3.connect('yes24_books.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS books\n",
    "                      (title TEXT, author TEXT, publisher TEXT, pub_date TEXT, price TEXT, discount TEXT, \n",
    "                      rating TEXT, review_count TEXT, detail_url TEXT, description TEXT)''')\n",
    "    \n",
    "    # Scrape all pages and insert data into the database\n",
    "    for page in range(1, total_pages + 1):\n",
    "        url = base_url.format(page)\n",
    "        try:\n",
    "            books_data = scrape_page(url)\n",
    "            cursor.executemany('INSERT INTO books VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)', books_data)\n",
    "            conn.commit()\n",
    "            print(f\"Page {page} scraped and saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while scraping page {page}: {str(e)}\")\n",
    "        time.sleep(random.uniform(1, 3))  # Random wait time between 1 and 3 seconds\n",
    "    \n",
    "    # Close the database connection\n",
    "    conn.close()\n",
    "    \n",
    "    # Read the data into a pandas DataFrame\n",
    "    conn = sqlite3.connect('yes24_books.db')\n",
    "    df = pd.read_sql_query(\"SELECT * FROM books\", conn)\n",
    "    conn.close()\n",
    "    \n",
    "    # Print the number of records and first few rows\n",
    "    print(f\"Total number of records: {len(df)}\")\n",
    "    print(\"\\nFirst few rows of the data:\")\n",
    "    print(df.head())\n",
    "\n",
    "category_dict = {\"컴퓨터공학\":\"001001003031\", \"프로그래밍 언어\":\"001001003022\", \"인공지능\":\"001001003032\"}\n",
    "for cat_id in category_dict.values():\n",
    "    main(cat_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50900dc0-4f44-48f9-a395-9b7fad6879a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('yes24_books.db')\n",
    "df = pd.read_sql_query(\"SELECT * FROM books\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d8671f-6652-406c-8f3c-179f26429c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb90e14-4dbc-4706-8325-3834af976bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e0238-c045-47ad-bc62-3e5ce8458935",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review_count\"] = pd.to_numeric(df[\"review_count\"].str.replace(\",\", \"\"), errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aec970-a6a0-4318-b8f4-ebf85ffa0d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nlargest(10, \"review_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c705f7a-318d-4d48-bf16-899ae2f883f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58b366d-219a-47f9-b833-73fdcced4907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
