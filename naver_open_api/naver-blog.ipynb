{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cae156-a554-4bb9-86cb-1b468295a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from urllib.parse import quote\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()  # 현재 프로젝트 폴더(또는 지정한 경로)에 있는 .env 파일 로드\n",
    "# 네이버 개발자 센터에서 발급받은 값으로 변경하세요\n",
    "client_id = os.getenv('NAVER_CLIENT_ID')\n",
    "client_secret = os.getenv('NAVER_CLIENT_SECRET')\n",
    "\n",
    "# 검색어\n",
    "search_query = \"LLM\"\n",
    "\n",
    "# 수집할 페이지 수 (예: 10페이지 -> 총 100개)\n",
    "num_pages = 50\n",
    "\n",
    "# 결과를 담을 리스트\n",
    "results = []\n",
    "\n",
    "for page in range(1, num_pages+1):\n",
    "    # 네이버 검색 API 문서에 따르면, start 파라미터는 1부터 시작하여\n",
    "    # 10씩 증가(예: 1, 11, 21, 31, ...)\n",
    "    start = (page - 1) * 10 + 1\n",
    "    print(page, end=\",\")\n",
    "\n",
    "    # 요청할 URL\n",
    "    url = \"https://openapi.naver.com/v1/search/blog\"\n",
    "\n",
    "    # 요청 헤더\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": client_id,\n",
    "        \"X-Naver-Client-Secret\": client_secret\n",
    "    }\n",
    "\n",
    "    # 요청 파라미터\n",
    "    params = {\n",
    "        \"query\": quote(search_query),  # 검색어\n",
    "        \"display\": 100,                 # 페이지당 문서 개수\n",
    "        \"start\": start,               # 검색 시작 위치\n",
    "        \"sort\": \"sim\"                 # 정렬 옵션(\"sim\": 정확도순, \"date\": 날짜순)\n",
    "    }\n",
    "\n",
    "    # GET 요청\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    \n",
    "    # 응답 결과 (JSON)\n",
    "    data = response.json()\n",
    "    \n",
    "    # 검색 결과의 'items' 키 아래에 결과 목록이 존재\n",
    "    items = data.get('items', [])\n",
    "\n",
    "    # items에서 필요한 데이터 추출 후 리스트에 저장\n",
    "    for item in items:\n",
    "        results.append({\n",
    "            \"title\": item[\"title\"],\n",
    "            \"link\": item[\"link\"],\n",
    "            \"description\": item[\"description\"],\n",
    "            \"bloggername\": item[\"bloggername\"],\n",
    "            \"bloggerlink\": item[\"bloggerlink\"],\n",
    "            \"postdate\": item[\"postdate\"]\n",
    "        })\n",
    "\n",
    "# 판다스 데이터프레임으로 변환\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# 결과 확인\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500c7e7-60ef-4a5c-8ce4-48b24584cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임을 CSV 파일로 저장하기\n",
    "csv_filename = \"naver_blog_search_result.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "# 저장한 CSV 파일을 불러와서 확인하기\n",
    "loaded_df = pd.read_csv(csv_filename)\n",
    "loaded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e9cfc-0bd6-4c16-8118-d5da243344bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# title, description, bloggername 세 개 컬럼을 공백으로 연결\n",
    "df['text'] = df['title'] + \" \" + df['description'] + \" \" + df['bloggername']\n",
    "vectorizer = TfidfVectorizer(\n",
    "    # max_features=1000,   # 필요한 경우 최대 피처 개수 제한\n",
    ")\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# ---------------------------\n",
    "# 토픽 모델링(LDA)\n",
    "# ---------------------------\n",
    "# n_components: 추출할 토픽(주제) 개수\n",
    "lda = LatentDirichletAllocation(n_components=4, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# 문서별 토픽 분포\n",
    "doc_topic_dists = lda.transform(X)  # shape: (문서 수, 토픽 개수)\n",
    "\n",
    "# 각 토픽별 단어 분포\n",
    "topic_word_dists = lda.components_  # shape: (토픽 개수, 단어(피처) 수)\n",
    "\n",
    "# 토픽별 주요 단어 살펴보기\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        # 중요도가 높은(가중치가 큰) 단어 순으로 정렬\n",
    "        top_features_idx = topic.argsort()[::-1][:n_top_words]\n",
    "        top_features = [(feature_names[i], topic[i]) for i in top_features_idx]\n",
    "        print(f\"Topic #{idx}:\")\n",
    "        for word, score in top_features:\n",
    "            print(f\"  {word}: {score:.4f}\")\n",
    "        print()\n",
    "\n",
    "print(\"==== LDA 토픽별 상위 단어 ====\")\n",
    "print_top_words(lda, feature_names, 5)\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# 문서별 토픽 분포를 별도 데이터프레임으로 만들기\n",
    "# ---------------------------------------\n",
    "# 토픽 개수만큼 컬럼명 생성 (예: topic0, topic1, ...)\n",
    "topic_cols = [f\"topic{i}\" for i in range(doc_topic_dists.shape[1])]\n",
    "df_topic_dist = pd.DataFrame(doc_topic_dists, columns=topic_cols)\n",
    "\n",
    "# title 칼럼을 맨 앞에 추가\n",
    "df_topic_dist.insert(0, \"title\", df[\"title\"])\n",
    "\n",
    "# 결과 확인\n",
    "print(\"==== 문서별 토픽 분포 데이터프레임 ====\")\n",
    "df_topic_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911f400c-afc0-4a3e-8d0e-d739367f396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_dist[['topic0', 'topic1', 'topic2', 'topic3']].sum(axis=1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0911a8-f4d0-4f98-81f2-f08c29aca374",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_dist.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a8ff21-0a78-4d8a-98ec-6a79bf215b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 클러스터링(K-Means)\n",
    "# ---------------------------\n",
    "# n_clusters: 클러스터 개수\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# 각 문서(행)에 대한 군집(label)\n",
    "labels = kmeans.labels_\n",
    "df['cluster'] = labels\n",
    "\n",
    "# 결과 확인\n",
    "print(\"==== 클러스터링 결과 ====\")\n",
    "print(df[['title','cluster']])\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 클러스터별 상위 키워드 10개 출력\n",
    "# ---------------------------\n",
    "# (X는 (문서 수 x 단어 수) 형태의 TF-IDF 행렬)\n",
    "# 각 클러스터의 주요 단어를 찾기 위해\n",
    "# \"클러스터 소속 문서들의 TF-IDF를 평균\" 낸 뒤, 상위 키워드를 추출\n",
    "n_top_keywords = 10\n",
    "for cluster_id in range(kmeans.n_clusters):\n",
    "    # 클러스터에 속한 문서 인덱스\n",
    "    cluster_indices = np.where(labels == cluster_id)\n",
    "    # 해당 클러스터 문서들의 TF-IDF 행렬만 추출\n",
    "    cluster_docs_tfidf = X[cluster_indices]\n",
    "    # 문서들의 평균 TF-IDF (shape: (1, 피처수))\n",
    "    mean_tfidf = cluster_docs_tfidf.mean(axis=0)\n",
    "    # sparse -> numpy array 변환\n",
    "    mean_tfidf = mean_tfidf.A1\n",
    "    \n",
    "    # 평균 TF-IDF가 큰 순으로 정렬 후 상위 n_top_keywords 인덱스 추출\n",
    "    top_indices = mean_tfidf.argsort()[::-1][:n_top_keywords]\n",
    "    top_features = feature_names[top_indices]\n",
    "    top_scores = mean_tfidf[top_indices]\n",
    "    \n",
    "    print(f\"\\n=== 클러스터 {cluster_id}의 상위 키워드 {n_top_keywords}개 ===\")\n",
    "    for rank, (feature, score) in enumerate(zip(top_features, top_scores), start=1):\n",
    "        print(f\"{rank}. {feature} (TF-IDF: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34b5cb8-5c66-40ae-b8b1-2568ae91f44d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
